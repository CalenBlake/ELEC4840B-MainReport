\chapter*{Abstract}
The classification of emotion in speech is a pertinent challenge for humanity and underpins the future of human-machine interaction. Despite the nature of such a problem existing within the sound processing realm, many modern methods utilise an image-processing approach through cross-domain transformations. This technique benefits from the ubiquity and complexity of soundly trained image classification networks. This paper introduces and explores the CyTex transform, a means of converting from the acoustic domain to an RGB textured image, without the loss of information \cite{CyTexRef}. The transform only relies upon prior knowledge of a signals fundamental frequency, which is readily obtainable through signal processing strategies. Extensions to advanced deep learning models are employed to classify the emotional content of speech signals through image classification. \\ \\
After training and comparing a number of models on the EMODB and RAVDESS datasets, the ResNet18 architecture was selected \cite{he2015deep}. An extended model, fine-tuned to CyTex data, was evaluated on both datasets. An average test accuracy of $70.27\%$ was achieved for the EMODB dataset when classifying across seven different emotional states. The same model was trained and evaluated using the RAVDESS dataset and achieved an average  classification accuracy of $56.87\%$ over a set of six emotions. All software resources used to acquire these results are included in the GitHub repository for this thesis \cite{Blake_ELEC4840B-Repo}.\\ \\
% After introducing the transform and its motivations, this thesis builds the foundational knowledge required for speech processing. Contemporary methods are discussed as well as the finer detail of the CyTex method itself. Finally, the future direction of the research is considered.