%   ****************************************
%   * File: 22S2FYP.BIB                    *
%   ****************************************
%   * An invented bib file                 *
%   * For the sample texts                 *
%   * The order is unimportant and there   *
%   * may be more entries than references  *
%   * in the text                          *
%   * Based off SAMPLE.BIB !!!             *
%   ****************************************
% 

@online{2decs,
  author = {Bjorn W. Schuller},
  title = {Speech emotion recognition: two decades in a nutshell, benchmarks, and ongoing trends.},
  year = {2018},
  url = {https://dl.acm.org/doi/10.1145/3129340},
  urldate = {2022-10-22}
}
@ARTICLE{imgprocsurveyOLD,
	AUTHOR = {Azriel Rosenfeld},
	TITLE = {Picture Processing by Computer},
	YEAR = {1969},
	JOURNAL = {ACM Computing Surveys},
	VOLUME = {1},
	PAGES = {147-176}
}
% Ali & co's original CyTex article
@ARTICLE{CyTexRef,
	AUTHOR = {A. Bakshi and A. Harami and S. Chalup},
	TITLE = {CyTex: Transforming speech to textured images for speech emotion recognition},
	YEAR = {2022},
	JOURNAL = {Speech Communication},
	VOLUME = {139},
	PAGES = {62-75}
}
@ARTICLE{lyingPsych,
    author = {Charles F. Bond, Jr. and Bella M. DePaulo},
    title ={Accuracy of Deception Judgments},
    year = {2006},
    journal = {Personality and Social Psychology Review},
    volume = {10},
    pages = {214-234}
}
@inproceedings{campbell_2000_databases,
    title={Databases of emotional speech},
    booktitle={ISCA tutorial and research workshop (ITRW) on speech and emotion},
    author={Campbell, Nick},
    year={September 5-7, 2000},
    address = {Newcastle, Northern Ireland, UK},
    url = {https://www.isca-speech.org/archive_open/speech_emotion/spem_034.html}
}
@article{shaver1987emotion,
    title={Emotion knowledge: further exploration of a prototype approach.},
    author={Shaver, Phillip and Schwartz, Judith and Kirson, Donald and O'connor, Cary},
    journal={Journal of personality and social psychology},
    volume={52},
    number={6},
    pages={1061-1086},
    year={1987},
    publisher={American Psychological Association}
}
@inproceedings{SUSAS_1997_DB,
    title={Getting started with SUSAS: a speech under simulated and actual stress database.},
    booktitle={Fifth European Conference on Speech Communication and Technology},
    author={Hansen, John HL and Bou-Ghazale, Sahar E and Sarikaya, Ruhi and Pellom, Bryan},
    year={September 22-25, 1997},
    address = {Rhodes, Greece},
    url = {https://www.isca-speech.org/archive_v0/archive_papers/eurospeech_1997/e97_1743.pdf}
}
@article{scherer1981speech,
  title={Speech and emotional states},
  author={Scherer, Klaus R},
  journal={Speech evaluation in psychiatry},
  pages={189--220},
  year={1981}
}
% Ysobel's recommended zero-shot isolation paper:
@inproceedings{chen2022zero,
    title={Zero-shot audio source separation through query-based learning from weakly-labeled data},
    author={Chen, Ke and Du, Xingjian and Zhu, Bilei and Ma, Zejun and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
    booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
    volume={36},
    number={4},
    pages={4441--4449},
    year={2022}
}
% IEEE 1975 Paper on processing speech signals
@ARTICLE{IEEE75_speechProccessing,
    author={Schafer, R.W. and Rabiner, L.R.},
    journal={Proceedings of the IEEE}, 
    title={Digital representations of speech signals}, 
    year={1975},
    volume={63},
    number={4},
    pages={662-677},
    doi={10.1109/PROC.1975.9799}
}
% Speech Processing - Dynamic and Optimization Approach (2018)
@Inbook{SP_03_textbook,
    author={Deng, L. and O'Shaughnessy, D.},
    title={Discrete-Time Signals, Systems, and Transforms},
    bookTitle={Speech Processing: A Dynamic and Optimization-Oriented Approach},
    year={2003},
    publisher={CRC Press},
    address={Boca Raton, Florida},
    doi={https://doi-org.ezproxy.newcastle.edu.au/10.1201/9781482276237}
}
% Pytorch Ref
@Inbook{PyTorch_Ketkar_2017,
    author={Ketkar, Nikhil},
    title={Introduction to PyTorch},
    bookTitle={Deep Learning with Python: A Hands-on Introduction},
    year={2017},
    publisher={Apress},
    address={Berkeley, CA},
    pages={195-208},
    isbn={978-1-4842-2766-4},
    doi={10.1007/978-1-4842-2766-4_12},
    url={https://doi.org/10.1007/978-1-4842-2766-4_12}
}
% Librosa Google Scholar paper 2015
@inproceedings{mcfee2015librosa,
    title={librosa: Audio and music signal analysis in python},
    author={McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel P and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
    booktitle={Proceedings of the 14th python in science conference},
    volume={8},
    pages={18--25},
    year={2015}
}
% Stanford Sinusoidal Peak Interpolation -> Used in librosa STFT
@BOOK{SASPWEB2011_stanford,
    AUTHOR = "Julius O. Smith",
    TITLE = "Spectral Audio Signal Processing",
    PUBLISHER = "\htmladdnormallink{\texttt{http:}}{http://ccrma.stanford.edu/~jos/sasp/}\texttt{//\-ccrma.stanford.edu/\-\~{}jos/\-sasp/}",
    YEAR = "2011",
    NOTE = "online book, 2011 edition"
}
% ======================= Contemporary Methods =======================
% Zhang 2019, LSTM 1 *** May not be necessary
@article{zhang2019spontaneous,
    title={Spontaneous speech emotion recognition using multiscale deep convolutional LSTM},
    author={Zhang, Shiqing and Zhao, Xiaoming and Tian, Qi},
    journal={IEEE Transactions on Affective Computing},
    year={2019},
    publisher={IEEE}
}
% Shirani and Nilchi 2016, => 87.21% acc on EMODB
@article{shirani2016speech,
    title={Speech emotion recognition based on SVM as both feature selector and classifier},
    author={Shirani, Amirreza and Nilchi, Ahmad Reza Naghsh},
    journal={International Journal of Image, Graphics and Signal Processing},
    volume={8},
    number={4},
    pages={39},
    year={2016},
    publisher={Modern Education and Computer Science Press}
}
% Milton and Selvi 2014, => 83.73% acc on EMODB
@article{MILTON2014,
    title = {Class-specific multiple classifiers scheme to recognize emotions from speech signals},
    journal = {Computer Speech & Language},
    volume = {28},
    number = {3},
    pages = {727-742},
    year = {2014},
    issn = {0885-2308},
    doi = {https://doi.org/10.1016/j.csl.2013.08.004},
    url = {https://www.sciencedirect.com/science/article/pii/S0885230813000594},
    author = {A. Milton and S. {Tamil Selvi}},
    keywords = {Multiple classifiers, Class specific classification, Classifier fusion, Speech emotion recognition, AR parameters}
}
% Sun et al 2015, => 81.74% EMODB
@article{SUN2015,
    title = {Weighted spectral features based on local Hu moments for speech emotion recognition},
    journal = {Biomedical Signal Processing and Control},
    volume = {18},
    pages = {80-90},
    year = {2015},
    issn = {1746-8094},
    doi = {https://doi.org/10.1016/j.bspc.2014.10.008},
    url = {https://www.sciencedirect.com/science/article/pii/S174680941400158X},
    author = {Yaxin Sun and Guihua Wen and Jiabing Wang},
    keywords = {Speech emotion recognition, Speech spectral features, Feature extraction, Hu moments}
}
% Yenigalla et al 2018, => 73.90% IEMOCAP
@inproceedings{yenigalla18_interspeech,
    author={Promod Yenigalla and Abhay Kumar and Suraj Tripathi and Chirag Singh and Sibsambhu Kar and Jithendra Vepa},
    title={{Speech Emotion Recognition Using Spectrogram & Phoneme Embedding}},
    year=2018,
    booktitle={Proc. Interspeech 2018},
    pages={3688-3692},
    doi={10.21437/Interspeech.2018-1811}
}
% ===================================================================

% Windowing functions ref
@article{heinzel2002spectrum,
    title={Spectrum and spectral density estimation by the Discrete Fourier transform (DFT), including a comprehensive list of window functions and some new at-top windows},
    author={Heinzel, Gerhard and R{\"u}diger, Albrecht and Schilling, Roland},
    year={2002}
}
% Introduction to Digital Speech Processing
@article{introtoDSP_core2,
    url = {http://dx.doi.org/10.1561/2000000001},
    year = {2007},
    volume = {1},
    journal = {Foundations and Trends® in Signal Processing},
    title = {Introduction to Digital Speech Processing},
    doi = {10.1561/2000000001},
    issn = {1932-8346},
    number = {1–2},
    pages = {1-194},
    author = {Lawrence R. Rabiner and Ronald W. Schafer}
}
% 1D convolution result
@article{zhang2017speech,
  title={Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching},
  author={Zhang, Shiqing and Zhang, Shiliang and Huang, Tiejun and Gao, Wen},
  journal={IEEE Transactions on Multimedia},
  volume={20},
  number={6},
  pages={1576--1590},
  year={2017},
  publisher={IEEE}
}

% ================= Transfer Learning =================
% How transferable are features in deep neural networks?
@article{yosinski2014transferable,
  title={How transferable are features in deep neural networks?},
  author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
% A survey of transfer learning
@article{2016transfer_survey,
  title={A survey of transfer learning},
  author={Weiss, Karl and Khoshgoftaar, Taghi M and Wang, DingDing},
  journal={Journal of Big data},
  volume={3},
  number={1},
  pages={1--40},
  year={2016},
  publisher={SpringerOpen}
}
% difference in data distribution result
@article{SHIMODAIRA2000,
    title = {Improving predictive inference under covariate shift by weighting the log-likelihood function},
    journal = {Journal of Statistical Planning and Inference},
    volume = {90},
    number = {2},
    pages = {227-244},
    year = {2000},
    issn = {0378-3758},
    doi = {https://doi.org/10.1016/S0378-3758(00)00115-4},
    url = {https://www.sciencedirect.com/science/article/pii/S0378375800001154},
    author = {Hidetoshi Shimodaira}
}
% IEEE Transfer Learning Survey
@ARTICLE{IEEE_TL_Survey,
  author={Pan, Sinno Jialin and Yang, Qiang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Transfer Learning}, 
  year={2010},
  volume={22},
  number={10},
  pages={1345-1359},
  doi={10.1109/TKDE.2009.191}}

% ===================== Databases =====================
% emodb report
@inproceedings{EMODB_05b_doc,
    author={Felix Burkhardt and A.Paeschke and M.Rolfes and Walter F.Sendlmeier and Benjamin Weiss},
    title={{A database of German emotional speech}},
    year={2005},
    booktitle={Proc. Interspeech 2005},
    pages={1517-1520},
    doi={10.21437/Interspeech.2005-446}
}
%emodb online documentation
@online{EMODB_97,
  author = {Burkhardt, Felix and Kienast, Miriam and Paeschke, Astrid and Weiss, Benjamin},
  title = {Berlin Database of Emotional Speech},
  year = {1997},
  url = {http://emodb.bilderbar.info/docu/#docu},
  urldate = {2022-10-26}
}
% RAVDESS Dataset
@dataset{ravdess_dataset_2018,
  author = {Livingstone, Steven R. and Russo, Frank A.},
  title = {{The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)}},
  month = apr,
  year = 2018,
  publisher = {Zenodo},
  version = {1.0.0},
  doi = {10.5281/zenodo.1188976},
  url = {https://doi.org/10.5281/zenodo.1188976}
}
% RAVDESS report
@article{ravdess_journal,
    doi = {10.1371/journal.pone.0196391},
    author = {Livingstone, Steven R. AND Russo, Frank A.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English},
    year = {2018},
    month = {05},
    volume = {13},
    url = {https://doi.org/10.1371/journal.pone.0196391},
    pages = {1-35},
    number = {5},
}
% RAVDESS intensity ref
@article{sonnemans1994structure,
  title={The structure of subjective emotional intensity},
  author={Sonnemans, Joep and Frijda, Nico H},
  journal={Cognition \& Emotion},
  volume={8},
  number={4},
  pages={329--350},
  year={1994},
  publisher={Taylor \& Francis}
}
% IEMOCAP documentation
@article{IEMOCAP_doc,
  title={IEMOCAP: Interactive emotional dyadic motion capture database},
  author={Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},
  journal={Language resources and evaluation},
  volume={42},
  number={4},
  pages={335-359},
  year={2008},
  publisher={Springer}
}
% IEMOCAP - scenario study
@book{scherer1986experiencing,
  title={Experiencing emotion: A cross-cultural study},
  author={Scherer, Klaus R and Wallbott, Harald G and Summerfield, Angela B},
  year={1986},
  publisher={Cambridge University Press}
}

% ==================== Extensions ====================
% ESC-50 Database
@inproceedings{piczak2015dataset,
  title = {{ESC}: {Dataset} for {Environmental Sound Classification}},
  author = {Piczak, Karol J.},
  booktitle = {Proceedings of the 23rd {Annual ACM Conference} on {Multimedia}},
  date = {2015-10-13},
  year = {2015},
  url = {http://dl.acm.org/citation.cfm?doid=2733373.2806390},
  doi = {10.1145/2733373.2806390},
  location = {{Brisbane, Australia}},
  isbn = {978-1-4503-3459-4},
  publisher = {{ACM Press}},
  pages = {1015--1018}
}
% IRMAS DATASET - MUSIC
@inproceedings{bosch2012comparison,
  title={A Comparison of Sound Segregation Techniques for Predominant Instrument Recognition in Musical Audio Signals.},
  author={Bosch, Juan J and Janer, Jordi and Fuhrmann, Ferdinand and Herrera, Perfecto},
  booktitle={ISMIR},
  pages={559--564},
  year={2012}
}
% Great Textbook resource for all things music classification
@article{won2021music,
  title={Music Classification: Beyond Supervised Learning, Towards Real-world Applications},
  author={Won, Minz and Spijkervet, Janne and Choi, Keunwoo},
  journal={arXiv preprint arXiv:2111.11636},
  year={2021}
}
% BIRDVOX-DCASE-20K
@dataset{lostanlen_vincent_2018_1208080,
  author       = {Lostanlen, Vincent and
                  Salamon, Justin and
                  Farnsworth, Andrew and
                  Kelling, Steve and
                  Bello, Juan Pablo},
  title        = {{BirdVox-DCASE-20k: a dataset for bird audio 
                   detection in 10-second clips}},
  month        = mar,
  year         = 2018,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.1208080},
  url          = {https://doi.org/10.5281/zenodo.1208080}
}
% ==================== Deep Learning ====================
% learning quote resource
@book{mitchell_97_ML,
  title={Machine learning},
  author={Mitchell, Tom Michael and others},
  volume={1},
  year={1997},
  publisher={McGraw-Hill Science/Engineering/Math}
}
% deep learn textbook (MIT)
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

% ==================== SER Results and Discussion ====================
% My GitHub Repo
@misc{Blake_ELEC4840B-Repo,
    author = {Blake, Calen},
    month = June,
    title = {{ELEC4840B-CodeRepository-CB}},
    note = {version 1.0},
    year = {2023},
    howpublished = {https://github.com/CalenBlake/ELEC4840B}
}
% Librosa package - VERSION 0.8.1 SPECIFIC
@software{brian_mcfee_2021_4782663,
  author       = {Brian McFee and
                  Alexandros Metsai and
                  Matt McVicar and
                  Stefan Balke and
                  Carl Thomé and
                  Colin Raffel and
                  Frank Zalkow and
                  Ayoub Malek and
                  Dana and
                  Kyungyun Lee and
                  Oriol Nieto and
                  Dan Ellis and
                  Jack Mason and
                  Eric Battenberg and
                  Scott Seyfarth and
                  Ryuichi Yamamoto and
                  viktorandreevichmorozov and
                  Keunwoo Choi and
                  Josh Moore and
                  Rachel Bittner and
                  Shunsuke Hidaka and
                  Ziyao Wei and
                  nullmightybofo and
                  Darío Hereñú and
                  Fabian-Robert Stöter and
                  Pius Friesch and
                  Adam Weiss and
                  Matt Vollrath and
                  Taewoon Kim and
                  Thassilo},
  title        = {librosa/librosa: 0.8.1rc1},
  month        = may,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {0.8.1rc1},
  doi          = {10.5281/zenodo.4782663},
  url          = {https://doi.org/10.5281/zenodo.4782663}
}
% original ResNet paper
@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
% Image Net OG Paper
@INPROCEEDINGS{deng2009imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}
}
% Reducing overfitting injection paper for cnns
@inproceedings{Thanapol2020,
    author = {Thanapol, Panissara and Lavangnananda, Kittichai and Bouvry, Pascal and Pinel, Frédéric and Leprevost, Franck},
    year = {2020},
    month = {10},
    pages = {300-305},
    title = {Reducing Overfitting and Improving Generalization in Training Convolutional Neural Network (CNN) under Limited Sample Sizes in Image Recognition},
    doi = {10.1109/InCIT50588.2020.9310787}
}
  
% ==================== LITERATURE REVIEW ====================
% Recognition of Emotion from Speech: A Review (2012)
  @incollection{Ramakrishnan12,
    author = {S. Ramakrishnan},
    title = {Recognition of Emotion from Speech: A Review},
    booktitle = {Speech Enhancement, Modeling and Recognition- Algorithms and Applications},
    publisher = {IntechOpen},
    address = {Rijeka},
    year = {2012},
    editor = {S. Ramakrishnan},
    chapter = {7},
    doi = {10.5772/39246},
    url = {https://doi.org/10.5772/39246}
}
% Speech emotion recognition: COMPLETE SUMMARY
@ARTICLE{surveyCORE1,
	AUTHOR = {M. B. Akçay and K. Oğuz},
	TITLE = {Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers},
	YEAR = {2020},
	JOURNAL = {Speech Communication},
	VOLUME = {116},
	PAGES = {56-76}
}
% For discrete emotion theory
@book{ekman2013emotion,
  title={Emotion in the human face: Guidelines for research and an integration of findings},
  author={Ekman, Paul and Friesen, Wallace V and Ellsworth, Phoebe},
  volume={11},
  year={2013},
  publisher={Elsevier}
}
% For dimensional emotion model
@article{RUSSELL1977,
    title = {Evidence for a three-factor theory of emotions},
    journal = {Journal of Research in Personality},
    volume = {11},
    number = {3},
    pages = {273-294},
    year = {1977},
    issn = {0092-6566},
    doi = {https://doi.org/10.1016/0092-6566(77)90037-X},
    url = {https://www.sciencedirect.com/science/article/pii/009265667790037X},
    author = {James A Russell and Albert Mehrabian}
}
% For info on Arousal
@article{nicolaou2011,
author = {Nicolaou, Mihalis and Gunes, Hatice and Pantic, Maja},
year = {2011},
month = {07},
pages = {92 - 105},
title = {Continuous Prediction of Spontaneous Affect from Multiple Cues and Modalities in Valence-Arousal Space},
volume = {2},
journal = {Affective Computing, IEEE Transactions on},
doi = {10.1109/T-AFFC.2011.9}
}
% Superiority of prosodic features
@ARTICLE{zeng2009,
  author={Zeng, Zhihong and Pantic, Maja and Roisman, Glenn I. and Huang, Thomas S.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions}, 
  year={2009},
  volume={31},
  number={1},
  pages={39-58},
  doi={10.1109/TPAMI.2008.52}}

% ==================== CONTEMPORARY METHODS ====================
% Zhao 2019, LSTM 1D and 2D (Biomed) => 95.33% acc on EMMODB
  @article{ZHAO2019,
    title = {Speech emotion recognition using deep 1D \& 2D CNN LSTM networks},
    journal = {Biomedical Signal Processing and Control},
    volume = {47},
    pages = {312-323},
    year = {2019},
    issn = {1746-8094},
    doi = {https://doi.org/10.1016/j.bspc.2018.08.035},
    url = {https://www.sciencedirect.com/science/article/pii/S1746809418302337},
    author = {Jianfeng Zhao and Xia Mao and Lijiang Chen},
    keywords = {Speech emotion recognition, CNN LSTM network, Raw audio clips, Log-mel spectrograms}
}
% 3d cnn
@misc{kim2017speech,
      title={Learning spectro-temporal features with 3D CNNs for speech emotion recognition}, 
      author={Jaebok Kim and Khiet P. Truong and Gwenn Englebienne and Vanessa Evers},
      year={2017},
      eprint={1708.05071},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
% TIM_NET
@misc{ye2023temporal,
      title={Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition}, 
      author={Jiaxin Ye and Xin-cheng Wen and Yujie Wei and Yong Xu and Kunhong Liu and Hongming Shan},
      year={2023},
      eprint={2211.08233},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
% CNN-Assisted Enhanced Audio Signal Processing - IEMOCAP RESULT
@Article{kwon_cnn-assist,
    AUTHOR = {Mustaqeem and Kwon, Soonil},
    TITLE = {A CNN-Assisted Enhanced Audio Signal Processing for Speech Emotion Recognition},
    JOURNAL = {Sensors},
    VOLUME = {20},
    YEAR = {2020},
    NUMBER = {1},
    ARTICLE-NUMBER = {183},
    URL = {https://www.mdpi.com/1424-8220/20/1/183},
    PubMedID = {31905692},
    ISSN = {1424-8220},
    DOI = {10.3390/s20010183}
}
% Shahzadi 2013, => 86.90% acc on EMODB
@article{shahzadi2013recognition,
    title={Recognition of emotion in speech using spectral patterns},
    author={Shahzadi, Ali and Ahmadyfard, Alireza and Yaghmaie, Khashayar and Harimi, Ali},
    journal={Malaysian Journal of Computer Science},
    volume={26},
    number={2},
    pages={140-158},
    year={2013}
}
% Zhou et al 2016, => 65% EMODB
@INPROCEEDINGS{zhou2016deep,
  author={Zhou, Xi and Guo, Junqi and Bie, Rongfang},
  booktitle={2016 Intl IEEE Conferences on Ubiquitous Intelligence \& Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)}, 
  title={Deep Learning Based Affective Model for Speech Emotion Recognition}, 
  year={2016},
  pages={841-846},
  organization={IEEE},
  doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0133}
}
% Speaker independent SVM approach
@article{Shirani2016,
    author = {Shirani, Reza and Naghsh-Nilchi, Ahmad},
    year = {2016},
    month = {04},
    pages = {39-45},
    title = {Speech Emotion Recognition based on SVM as Both Feature Selector and Classifier},
    volume = {8},
    journal = {International Journal of Image, Graphics and Signal Processing},
    doi = {10.5815/ijigsp.2016.04.05}
}
